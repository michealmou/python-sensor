================================================================================
                    ğŸ¤Ÿ SIGNSENSE â€” FULL PROJECT ROADMAP
            Real-Time Sign Language Interpreter (ASL â†’ Text/Speech)
                 By Michael Musallam and Nadim Baboun
================================================================================

Created: February 27, 2026
Project: python-sensor (SignSense)

================================================================================
TABLE OF CONTENTS
================================================================================

  1. WHERE WE ARE NOW (Current State Assessment)
  2. PROJECT GOALS (Short-Term & Long-Term)
  3. DEVELOPMENT ROADMAP (Step-by-Step Phases)
  4. COMPLETE LIBRARY & TECHNOLOGY REFERENCE
  5. PRACTICAL TIPS & TRICKS
  6. TUTORIALS, DOCUMENTATION & RESOURCES
  7. ARCHITECTURE OVERVIEW

================================================================================
1. WHERE WE ARE NOW â€” CURRENT STATE ASSESSMENT
================================================================================

What we already have:
---------------------
âœ… Hand detection using MediaPipe (hand_detector.py)
   - Detects up to 2 hands in real-time
   - Extracts all 21 landmark positions (x, y pixel coordinates)
   - Distinguishes left vs. right hand

âœ… Webcam pipeline (main.py)
   - Camera initialization with fallback (tries indices 0â€“4)
   - Mirror-mode display
   - FPS counter
   - Frame capture loop at 1280x720

âœ… Gesture-based mouse control (mouse_controller.py)
   - Index finger â†’ mouse movement (with exponential smoothing)
   - Thumb + Index pinch â†’ left click
   - Thumb + Middle pinch â†’ right click
   - Thumb + Ring pinch â†’ scroll mode

âœ… Drawing utilities (utils/drawing_utils.py)
   - Hand landmark points with fingertip highlighting
   - Bounding box with label
   - MediaPipe skeleton visualization

âœ… Reference project explored (Handy-Sign-Language-Detection-main)
   - Image collection pipeline (img collect.py)
   - Landmark extraction â†’ data.pickle (landmarks.py)
   - RandomForest training (train.py)
   - Real-time classifier (classifier.py) with 10 signs

What we're MISSING for a full interpreter:
------------------------------------------
âŒ No sign language classification model integrated
âŒ No dataset collection pipeline
âŒ No feature extraction from landmarks
âŒ No trained ML model (just mouse gestures)
âŒ No text output of recognized signs
âŒ No text-to-speech for recognized signs
âŒ No word/sentence building from individual signs
âŒ No support for dynamic signs (motion-based signs like "help", "thank you")
âŒ No gesture history / temporal recognition
âŒ No UI overlay for displaying translations

================================================================================
2. PROJECT GOALS
================================================================================

SHORT-TERM GOALS (Weeks 1â€“4):
------------------------------
  [S1] Build a dataset of ASL signs using our webcam
  [S2] Extract hand landmarks into a structured dataset
  [S3] Train a classification model (RandomForest â†’ then upgrade)
  [S4] Integrate real-time sign prediction into our existing pipeline
  [S5] Display the predicted sign/letter on the camera feed
  [S6] Support the 26 ASL alphabet letters (Aâ€“Z)
  [S7] Add confidence score display

MEDIUM-TERM GOALS (Weeks 5â€“10):
---------------------------------
  [M1] Expand vocabulary to common words/phrases (hello, yes, no, thank you, etc.)
  [M2] Add text-to-speech output (computer speaks the sign)
  [M3] Build word/sentence accumulation (spelling mode)
  [M4] Implement dynamic gesture recognition (signs involving motion)
  [M5] Add a proper UI overlay / HUD for translation display
  [M6] Switch from RandomForest to a neural network (better accuracy)
  [M7] Create a configuration/settings system (sensitivity, modes, etc.)

LONG-TERM GOALS (Weeks 11+):
------------------------------
  [L1] Two-hand sign recognition (signs requiring both hands)
  [L2] Continuous sign language recognition (not just individual signs)
  [L3] Support for multiple sign languages (ASL, BSL, ISL, etc.)
  [L4] Build a desktop GUI application using PyQt5/Tkinter
  [L5] Web-based version with camera access (Flask/FastAPI + WebSocket)
  [L6] Mobile app integration (optional, via React Native or Flutter)
  [L7] Use deep learning (LSTM/Transformer) for sentence-level recognition
  [L8] Real-time translation overlay using AR-style display

================================================================================
3. DEVELOPMENT ROADMAP â€” STEP-BY-STEP PHASES
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 0: PROJECT RESTRUCTURE                      â”‚
â”‚                          Duration: 1â€“2 days                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Reorganize the codebase for the sign language interpreter.

Step 0.1 â€” Create new project structure:
  python-sensor/
  â”œâ”€â”€ main.py                    # Main application (mode switching)
  â”œâ”€â”€ hand_detector.py           # [KEEP] Hand detection
  â”œâ”€â”€ mouse_controller.py        # [KEEP] Mouse control mode
  â”œâ”€â”€ sign_classifier.py         # [NEW] Sign language classifier
  â”œâ”€â”€ feature_extractor.py       # [NEW] Landmark â†’ feature vector
  â”œâ”€â”€ sentence_builder.py        # [NEW] Word/sentence accumulation
  â”œâ”€â”€ config.py                  # [NEW] Configuration constants
  â”œâ”€â”€ requirements.txt           # [UPDATE] All dependencies
  â”œâ”€â”€ utils/
  â”‚   â”œâ”€â”€ drawing_utils.py       # [KEEP] Drawing helpers
  â”‚   â””â”€â”€ text_overlay.py        # [NEW] Text display on frame
  â”œâ”€â”€ data/
  â”‚   â”œâ”€â”€ collect_images.py      # [NEW] Dataset collection script
  â”‚   â”œâ”€â”€ extract_landmarks.py   # [NEW] Landmark extraction script
  â”‚   â””â”€â”€ raw/                   # [NEW] Raw collected images by class
  â”œâ”€â”€ models/
  â”‚   â”œâ”€â”€ train_model.py         # [NEW] Training script
  â”‚   â””â”€â”€ saved/                 # [NEW] Saved model files (.p, .h5)
  â””â”€â”€ assets/
      â””â”€â”€ reference/             # Reference images of ASL alphabet

Step 0.2 â€” Create config.py with all magic numbers:
  - Camera resolution, frame reduction
  - Detection/tracking confidence thresholds
  - Model paths
  - Sign vocabulary dictionary

Step 0.3 â€” Add mode switching to main.py:
  - Mode 1: Mouse Control (current functionality)
  - Mode 2: Sign Language Interpreter (new)
  - Toggle with keyboard shortcut (e.g., press 'M' to switch)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PHASE 1: DATA COLLECTION PIPELINE                    â”‚
â”‚                          Duration: 3â€“5 days                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Collect a dataset of hand sign images from your webcam.

Step 1.1 â€” Create data/collect_images.py:
  - Open webcam
  - For each sign/letter (Aâ€“Z, plus common words):
      â†’ Show instruction: "Show sign for [X], press 'S' to start"
      â†’ Capture N images (start with 200â€“300 per sign)
      â†’ Save to data/raw/<sign_label>/img_001.jpg, img_002.jpg, ...
  - Add slight delay between captures for hand position variation
  - Show a live preview with countdown

  Key code pattern:
  -----------------------------------------------------------------
  import cv2, os

  DATA_DIR = './data/raw'
  NUM_CLASSES = 26        # Aâ€“Z initially
  IMAGES_PER_CLASS = 300

  cap = cv2.VideoCapture(0)
  for class_id in range(NUM_CLASSES):
      class_dir = os.path.join(DATA_DIR, str(class_id))
      os.makedirs(class_dir, exist_ok=True)

      # Wait for user to get ready
      while True:
          ret, frame = cap.read()
          cv2.putText(frame, f'Class {class_id} - Press "S" to start',
                      (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
          cv2.imshow('Collect', frame)
          if cv2.waitKey(1) & 0xFF == ord('s'):
              break

      # Capture images
      for img_num in range(IMAGES_PER_CLASS):
          ret, frame = cap.read()
          cv2.imshow('Collect', frame)
          cv2.imwrite(os.path.join(class_dir, f'{img_num}.jpg'), frame)
          cv2.waitKey(50)   # 50ms between captures
  -----------------------------------------------------------------

Step 1.2 â€” Data quality tips:
  - Vary hand position slightly between captures
  - Use different lighting conditions
  - Capture from different angles
  - Include both left and right hand samples
  - Keep background as clean as possible initially

Step 1.3 â€” Optional: Use the sign_mnist_train.csv dataset
  (Already on your system at c:\Users\Admin\Downloads\sign_mnist_train.csv)
  - This is the ASL MNIST dataset (28x28 grayscale images as CSV)
  - Good for initial prototyping and testing your pipeline
  - Contains Aâ€“Z excluding J and Z (motion letters)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PHASE 2: FEATURE EXTRACTION (LANDMARKS)                â”‚
â”‚                          Duration: 2â€“3 days                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Convert raw images into landmark-based feature vectors.

Step 2.1 â€” Create data/extract_landmarks.py:
  - Load each image from data/raw/
  - Run MediaPipe hand detection
  - Extract 21 landmarks (x, y) â†’ 42 values per hand
  - IMPORTANT: Normalize coordinates relative to hand bounding box
    (this makes features position-invariant)
  - Save as data/landmarks.pickle

  Key normalization pattern:
  -----------------------------------------------------------------
  # Instead of raw (x, y), normalize relative to hand bounding box:
  x_coords = [lm.x for lm in hand_landmarks.landmark]
  y_coords = [lm.y for lm in hand_landmarks.landmark]
  min_x, min_y = min(x_coords), min(y_coords)

  features = []
  for lm in hand_landmarks.landmark:
      features.append(lm.x - min_x)   # relative x
      features.append(lm.y - min_y)   # relative y

  # Optionally add z-coordinates for depth info (63 features):
  # features.append(lm.z)
  -----------------------------------------------------------------

Step 2.2 â€” Create feature_extractor.py (reusable module):
  - Class: FeatureExtractor
  - Method: extract(hand_landmarks) â†’ numpy array of features
  - Method: normalize(features) â†’ position-invariant features
  - This module will be used both in training AND real-time inference

Step 2.3 â€” Verify data quality:
  - Check that all classes have equal sample counts
  - Remove samples where MediaPipe failed to detect a hand
  - Print dataset statistics (total samples, per-class count)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 3: MODEL TRAINING                           â”‚
â”‚                          Duration: 3â€“5 days                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Train a machine learning model to classify signs from landmarks.

Step 3.1 â€” Start with RandomForest (quick baseline):
  - File: models/train_model.py
  - Load landmarks.pickle
  - Split: 80% train / 20% test (stratified)
  - Train RandomForestClassifier(n_estimators=100)
  - Print accuracy score
  - Save model to models/saved/model_rf.p

  Expected baseline accuracy: 85â€“95% for static signs

Step 3.2 â€” Evaluate and iterate:
  - Print confusion matrix (which signs get confused?)
  - Identify problem classes â†’ collect more data for them
  - Try hyperparameter tuning:
      n_estimators: [50, 100, 200]
      max_depth: [None, 10, 20, 30]

Step 3.3 â€” Upgrade to neural network (optional, for better accuracy):
  - Use scikit-learn MLPClassifier or TensorFlow/Keras
  - Architecture: Input(42) â†’ Dense(128, relu) â†’ Dense(64, relu) â†’ Dense(N, softmax)
  - Train for 50â€“100 epochs
  - Compare accuracy with RandomForest

  Key pattern for Keras model:
  -----------------------------------------------------------------
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Dense, Dropout

  model = Sequential([
      Dense(128, activation='relu', input_shape=(42,)),
      Dropout(0.3),
      Dense(64, activation='relu'),
      Dropout(0.3),
      Dense(num_classes, activation='softmax')
  ])
  model.compile(optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
  model.fit(X_train, y_train, epochs=50, validation_split=0.2)
  model.save('models/saved/model_nn.h5')
  -----------------------------------------------------------------

Step 3.4 â€” Save label mapping:
  - Create a labels_dict: {0: "A", 1: "B", 2: "C", ...}
  - Save alongside model for inference


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PHASE 4: REAL-TIME CLASSIFICATION                      â”‚
â”‚                          Duration: 3â€“4 days                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Integrate the trained model into the live webcam pipeline.

Step 4.1 â€” Create sign_classifier.py:
  -----------------------------------------------------------------
  import pickle
  import numpy as np

  class SignClassifier:
      def __init__(self, model_path='models/saved/model_rf.p'):
          model_dict = pickle.load(open(model_path, 'rb'))
          self.model = model_dict['model']
          self.labels = {0: "A", 1: "B", ...}  # Load from file

      def predict(self, features):
          """
          features: numpy array of shape (42,)
          Returns: (predicted_label, confidence)
          """
          prediction = self.model.predict([features])
          # For RandomForest, get probability:
          probabilities = self.model.predict_proba([features])
          confidence = np.max(probabilities)
          label = self.labels[int(prediction[0])]
          return label, confidence
  -----------------------------------------------------------------

Step 4.2 â€” Integrate into main.py:
  - In sign language mode:
      1. Get hand landmarks from HandDetector
      2. Extract features using FeatureExtractor
      3. Predict sign using SignClassifier
      4. Display result on frame (letter + confidence %)
  - Add a minimum confidence threshold (e.g., 70%)
  - Only show prediction when confidence > threshold

Step 4.3 â€” Add stability filtering:
  - Don't change displayed sign on every single frame
  - Keep a history of last N predictions (e.g., last 10 frames)
  - Only update display when the same sign appears in >60% of history
  - This prevents flickering between signs

  Key pattern:
  -----------------------------------------------------------------
  from collections import deque, Counter

  prediction_history = deque(maxlen=15)  # last 15 frames

  # In main loop:
  prediction_history.append(predicted_label)
  most_common = Counter(prediction_history).most_common(1)[0]
  if most_common[1] / len(prediction_history) > 0.6:
      stable_prediction = most_common[0]
  -----------------------------------------------------------------

Step 4.4 â€” Create utils/text_overlay.py:
  - Function: draw_prediction(frame, label, confidence, position)
  - Include: background rectangle, large text, confidence bar
  - Color-code by confidence (green = high, yellow = medium, red = low)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PHASE 5: TEXT-TO-SPEECH & SENTENCE BUILDING            â”‚
â”‚                          Duration: 3â€“4 days                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Build words from individual letters and speak them aloud.

Step 5.1 â€” Create sentence_builder.py:
  -----------------------------------------------------------------
  class SentenceBuilder:
      def __init__(self):
          self.current_word = ""
          self.sentence = ""
          self.last_sign = None
          self.sign_hold_start = None
          self.hold_threshold = 1.5  # seconds to "confirm" a letter

      def update(self, sign, timestamp):
          if sign == self.last_sign:
              # Same sign held â†’ check if threshold reached
              if timestamp - self.sign_hold_start >= self.hold_threshold:
                  self.current_word += sign
                  self.sign_hold_start = timestamp  # reset for next letter
          else:
              self.last_sign = sign
              self.sign_hold_start = timestamp

      def add_space(self):
          self.sentence += self.current_word + " "
          self.current_word = ""

      def get_display_text(self):
          return self.sentence + self.current_word
  -----------------------------------------------------------------

Step 5.2 â€” Add special gestures:
  - Open palm (5 fingers) â†’ SPACE (finish current word)
  - Fist (0 fingers) â†’ BACKSPACE (delete last letter)
  - Thumbs up â†’ SPEAK (trigger text-to-speech)
  - Two open palms â†’ CLEAR (reset sentence)

Step 5.3 â€” Integrate text-to-speech:
  - Use pyttsx3 (offline TTS engine)
  -----------------------------------------------------------------
  import pyttsx3

  engine = pyttsx3.init()
  engine.setProperty('rate', 150)  # Speed of speech

  def speak(text):
      engine.say(text)
      engine.runAndWait()
  -----------------------------------------------------------------

Step 5.4 â€” Display the accumulated text:
  - Show current word being built at top of frame
  - Show full sentence below it
  - Visual indicator for "hold to confirm" progress bar


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 6: DYNAMIC GESTURE RECOGNITION                â”‚
â”‚                         Duration: 5â€“7 days                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Recognize signs that involve hand MOTION (not just static poses).

Why: Many important signs (thank you, help, sorry, etc.) involve
     hand movement over time, not just a frozen hand position.

Step 6.1 â€” Collect temporal data:
  - Instead of single frames, capture SEQUENCES of landmarks
  - For each dynamic sign, record 30 frames (~1 second at 30 FPS)
  - Save as sequences: data/sequences/<sign>/<sample_N>.npy
  - Each sample shape: (30, 42) â†’ 30 frames Ã— 42 features

Step 6.2 â€” Use LSTM (Long Short-Term Memory) neural network:
  -----------------------------------------------------------------
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import LSTM, Dense, Dropout

  model = Sequential([
      LSTM(64, return_sequences=True, input_shape=(30, 42)),
      Dropout(0.3),
      LSTM(128, return_sequences=False),
      Dropout(0.3),
      Dense(64, activation='relu'),
      Dense(num_dynamic_signs, activation='softmax')
  ])
  model.compile(optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
  -----------------------------------------------------------------

Step 6.3 â€” Combine static + dynamic classifiers:
  - Run static classifier continuously for alphabet/static signs
  - When motion is detected (landmark velocity > threshold), switch to
    feeding frames into the LSTM sequence buffer
  - After 30 frames, run LSTM prediction
  - Display the result alongside static predictions


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PHASE 7: POLISHED UI & APPLICATION                 â”‚
â”‚                         Duration: 5â€“7 days                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: Build a professional-looking desktop application.

Step 7.1 â€” Design the HUD overlay:
  - Prediction display (current sign + confidence)
  - Sentence display area
  - Mode indicator (Mouse / Sign Language)
  - Mini reference card showing ASL alphabet
  - FPS and system status

Step 7.2 â€” Optional: Build a GUI with PyQt5 or Tkinter:
  - Camera feed in center
  - Text/sentence panel on the right
  - Settings panel (confidence threshold, speech rate, etc.)
  - Sign reference gallery at the bottom

Step 7.3 â€” Optional: Web version with Flask:
  - Stream webcam via WebSocket
  - Process frames server-side
  - Display results in browser
  - More accessible / shareable than desktop app


================================================================================
4. COMPLETE LIBRARY & TECHNOLOGY REFERENCE
================================================================================

Below is every library you will need, what it does, why we need it,
key concepts, and how we'll use it in this project.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.1  OpenCV (opencv-python)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Open Source Computer Vision Library (Python bindings)
Install:        pip install opencv-python>=4.8.0
What It Does:   Image and video processing. Capture, manipulate, and
                display video frames in real-time.
Why We Need It: Core of our pipeline â€” captures webcam feed, processes
                frames, draws overlays, and displays the output window.

Key Concepts:
  - VideoCapture      â†’ Opens camera/video input
  - imread / imwrite  â†’ Read/write image files
  - cvtColor          â†’ Convert color spaces (BGR â†” RGB)
  - flip              â†’ Mirror image
  - circle, rectangle, putText â†’ Draw on frames
  - imshow / waitKey  â†’ Display window and handle keyboard input
  - CAP_DSHOW         â†’ DirectShow backend (Windows cameras)

How We Use It:
  - Capture webcam frames in real-time (main.py)
  - Draw hand landmarks, bounding boxes, prediction text
  - Save images during data collection
  - Display the final output with all overlays
  - Handle keyboard shortcuts for mode switching

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.2  MediaPipe
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Google MediaPipe (ML framework for multimodal pipelines)
Install:        pip install mediapipe>=0.10.0
What It Does:   Pre-trained ML models for face, hand, and pose detection.
                Specifically, the Hands module detects 21 landmarks per hand.
Why We Need It: Heart of our hand tracking â€” provides the 21 landmark
                positions that we extract features from.

Key Concepts:
  - mp.solutions.hands      â†’ Hand detection module
  - Hands()                 â†’ The hand detector object
  - hand_landmarks.landmark â†’ List of 21 NormalizedLandmarks
  - HAND_CONNECTIONS        â†’ Skeleton connection pairs
  - static_image_mode       â†’ True for images, False for video
  - min_detection_confidence / min_tracking_confidence
  - NormalizedLandmark      â†’ Has .x, .y, .z (0.0â€“1.0 normalized)

  The 21 Landmarks:
    0: WRIST
    1â€“4: THUMB (CMC, MCP, IP, TIP)
    5â€“8: INDEX FINGER (MCP, PIP, DIP, TIP)
    9â€“12: MIDDLE FINGER (MCP, PIP, DIP, TIP)
    13â€“16: RING FINGER (MCP, PIP, DIP, TIP)
    17â€“20: PINKY (MCP, PIP, DIP, TIP)

How We Use It:
  - hand_detector.py uses it to detect hands and extract landmarks
  - Landmarks feed into FeatureExtractor â†’ then into the ML model
  - Also used during data collection to process saved images

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.3  NumPy
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Numerical Python
Install:        pip install numpy>=1.24.0
What It Does:   Fast numerical operations on arrays and matrices.
Why We Need It: Feature vectors and data manipulation â€” landmarks are
                stored as numpy arrays for efficient processing.

Key Concepts:
  - np.array / np.asarray  â†’ Create arrays
  - np.interp              â†’ Linear interpolation (coordinate mapping)
  - np.max, np.argmax      â†’ Find maximum values
  - Broadcasting           â†’ Automatic array shape matching
  - Vectorized operations  â†’ Fast batch math without loops
  - Shape and reshape      â†’ Array dimensionality

How We Use It:
  - Convert landmark lists to numpy arrays for ML model input
  - Coordinate interpolation (hand space â†’ screen space)
  - Feature normalization
  - Model prediction input formatting

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.4  scikit-learn (sklearn)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Scikit-Learn (Machine Learning in Python)
Install:        pip install scikit-learn>=1.3.0
What It Does:   Traditional ML algorithms, data splitting, evaluation,
                preprocessing, and model selection.
Why We Need It: Train our first sign classifier (RandomForest), evaluate
                accuracy, and handle data splitting.

Key Concepts:
  - RandomForestClassifier  â†’ Ensemble of decision trees (our baseline)
  - train_test_split        â†’ Split data into train/test sets
  - accuracy_score          â†’ Calculate model accuracy
  - confusion_matrix        â†’ See which classes get confused
  - classification_report   â†’ Precision, recall, F1 per class
  - predict_proba           â†’ Get confidence scores per class
  - cross_val_score         â†’ K-fold cross validation
  - StandardScaler          â†’ Normalize features to mean=0, std=1
  - MLPClassifier           â†’ Neural network alternative

How We Use It:
  - Train RandomForest on landmark features (Phase 3)
  - Evaluate model performance
  - predict_proba for confidence-based filtering
  - Compare model variants

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.5  TensorFlow / Keras  (Medium-Term)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      TensorFlow (with Keras high-level API)
Install:        pip install tensorflow>=2.13.0
What It Does:   Deep learning framework for building and training neural
                networks (Dense, LSTM, CNN, Transformer).
Why We Need It: More accurate sign classification, and REQUIRED for
                dynamic gesture recognition (LSTM sequences).

Key Concepts:
  - Sequential model        â†’ Stack of layers
  - Dense layer             â†’ Fully connected neurons
  - LSTM layer              â†’ Long Short-Term Memory (sequence data)
  - Dropout                 â†’ Regularization to prevent overfitting
  - Softmax activation      â†’ Output probabilities per class
  - Categorical crossentropyâ†’ Loss function for multi-class
  - Adam optimizer           â†’ Adaptive learning rate optimizer
  - model.fit()             â†’ Train the model
  - model.predict()         â†’ Run inference
  - model.save() / load_model() â†’ Save/load trained models

How We Use It:
  - Phase 3 (optional): Dense NN for static sign classification
  - Phase 6 (required): LSTM for dynamic gesture recognition
  - Replace RandomForest when higher accuracy is needed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.6  pyttsx3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Python Text-to-Speech version 3
Install:        pip install pyttsx3
What It Does:   Offline text-to-speech synthesis. Converts text strings
                into spoken audio using system TTS engines.
Why We Need It: Speak the recognized sign or built sentence aloud,
                making the interpreter accessible.

Key Concepts:
  - pyttsx3.init()          â†’ Initialize TTS engine
  - engine.say(text)        â†’ Queue text for speaking
  - engine.runAndWait()     â†’ Block until speech finishes
  - engine.setProperty('rate', N)   â†’ Set speech speed (words/min)
  - engine.setProperty('volume', N) â†’ Set volume (0.0â€“1.0)
  - engine.getProperty('voices')    â†’ List available voices

How We Use It:
  - When user does "thumbs up" gesture â†’ speak the current sentence
  - Optional: speak each letter as it's recognized
  - Runs offline, no internet needed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.7  PyAutoGUI
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      PyAutoGUI (Python GUI Automation)
Install:        pip install pyautogui
What It Does:   Programmatically control mouse and keyboard.
Why We Need It: Already used in our mouse control mode.

Key Concepts:
  - pyautogui.moveTo(x, y)     â†’ Move mouse to absolute position
  - pyautogui.click(button)    â†’ Click mouse
  - pyautogui.scroll(clicks)   â†’ Scroll mouse wheel
  - pyautogui.size()           â†’ Get screen resolution
  - pyautogui.position()       â†’ Get current mouse position
  - FAILSAFE / PAUSE           â†’ Safety settings

How We Use It:
  - Mouse control mode in main.py (already implemented)
  - Will remain as an alternative mode alongside sign language mode

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.8  pickle (built-in)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Python Object Serialization (built-in module)
Install:        No installation needed (part of Python standard library)
What It Does:   Serialize and deserialize Python objects to/from files.
Why We Need It: Save and load trained models, datasets, and label maps.

Key Concepts:
  - pickle.dump(obj, file)  â†’ Save object to file
  - pickle.load(file)       â†’ Load object from file
  - 'wb' / 'rb' modes      â†’ Write/read binary

How We Use It:
  - Save extracted landmark datasets (data.pickle)
  - Save trained models (model_rf.p)
  - Save label dictionaries

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.9  collections (built-in)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Python Collections Module (built-in)
Install:        No installation needed (part of Python standard library)
What It Does:   Specialized container data types.
Why We Need It: Prediction smoothing using deque and Counter.

Key Concepts:
  - deque(maxlen=N)         â†’ Fixed-size FIFO queue
  - Counter(iterable)       â†’ Count occurrences
  - Counter.most_common(N)  â†’ Get N most frequent items

How We Use It:
  - deque to store last N predictions
  - Counter to find the most frequent prediction (stability filter)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.10  Flask / FastAPI  (Long-Term, Optional)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      Flask (micro web framework) or FastAPI (async web framework)
Install:        pip install flask  OR  pip install fastapi uvicorn
What It Does:   Build web servers and APIs.
Why We Need It: If we want to build a web-based version of the interpreter.

Key Concepts:
  Flask:
    - @app.route           â†’ Define URL endpoints
    - render_template      â†’ Serve HTML pages
    - request / response   â†’ Handle HTTP data
  FastAPI:
    - @app.get / @app.post â†’ Define endpoints
    - WebSocket support    â†’ Real-time communication
    - Automatic API docs   â†’ Swagger UI

How We Use It:
  - Stream webcam to browser
  - Process frames on server
  - Send predictions back via WebSocket
  - Build a shareable, cross-platform interface

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.11  PyQt5 / Tkinter  (Long-Term, Optional)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Full Name:      PyQt5 (Qt for Python) or Tkinter (Tk GUI toolkit)
Install:        pip install PyQt5  (Tkinter is built-in)
What It Does:   Build native desktop GUI applications.
Why We Need It: Professional desktop app with panels, settings, etc.

Key Concepts:
  PyQt5:
    - QMainWindow, QWidget â†’ Window containers
    - QLabel              â†’ Display images/text
    - QTimer              â†’ Periodic updates (for video feed)
    - Signal/Slot         â†’ Event handling
  Tkinter:
    - Tk(), mainloop()    â†’ Main window
    - Canvas              â†’ Drawing area
    - Label, Button       â†’ Standard widgets
    - after()             â†’ Schedule periodic updates

How We Use It:
  - Camera feed display panel
  - Translation text panel
  - Settings/configuration panel
  - Sign reference gallery


================================================================================
  FULL requirements.txt (copy this when ready)
================================================================================

  # Core (already have)
  opencv-python>=4.8.0
  mediapipe>=0.10.0
  numpy>=1.24.0
  pyautogui

  # Machine Learning
  scikit-learn>=1.3.0

  # Deep Learning (install when reaching Phase 6)
  # tensorflow>=2.13.0

  # Text-to-Speech (install when reaching Phase 5)
  # pyttsx3

  # Web App (install only if building web version)
  # flask>=3.0.0
  # OR
  # fastapi>=0.100.0
  # uvicorn>=0.23.0

  # Desktop GUI (install only if building desktop app)
  # PyQt5>=5.15.0


================================================================================
5. PRACTICAL TIPS & TRICKS
================================================================================

GENERAL TIPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… Start small, iterate fast
    Don't try to recognize all 26 letters at once. Start with 5 letters
    (A, B, C, L, Y â€” they're visually distinct). Get end-to-end working,
    then expand.

  â˜… Test every phase independently
    Data collection â†’ verify images look correct.
    Landmark extraction â†’ visualize landmarks on images.
    Training â†’ check accuracy before integrating.
    Don't skip validation steps!

  â˜… Version control everything
    Commit after each phase. Use branches for experiments.
    Tag working milestones (e.g., v0.1-basic-classifier).

DATA COLLECTION TIPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… Quality > Quantity
    300 clean images per class > 1000 messy ones.
    Check your collected images manually.

  â˜… Augment your data
    Flip horizontally, slight rotation, brightness changes.
    This makes your model more robust. Use cv2 transformations.

  â˜… Use the sign_mnist_train.csv as a starting point
    You already have it! It's pre-processed and ready to go.
    Use it to build and test your training pipeline.

  â˜… Record at multiple distances
    Close up, medium, far â€” all valid hand positions.

MODEL TRAINING TIPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… Always normalize features
    Subtract the minimum x and y from all landmarks.
    This makes prediction position-invariant (hand can be anywhere).

  â˜… Check the confusion matrix
    It tells you which signs look alike to the model.
    Collect more data for confused classes.

  â˜… Start with RandomForest, upgrade later
    RandomForest is fast to train, needs no GPU, and gives good results.
    Only move to neural networks when you hit accuracy limits.

  â˜… Use predict_proba, not just predict
    Confidence scores let you filter out uncertain predictions.
    Set a threshold (e.g., 70%) â€” show "?" when below it.

REAL-TIME PERFORMANCE TIPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… Prediction smoothing is CRITICAL
    Without smoothing, the displayed sign flickers every frame.
    Use deque + Counter to show the most stable prediction.

  â˜… Don't run the model on every single frame
    Run prediction every 2nd or 3rd frame to save CPU.
    The hand position doesn't change much between frames.

  â˜… Keep the webcam resolution at 640x480 for training
    Lower resolution = faster processing during data collection.
    Use 1280x720 only for the final display.

  â˜… Profile your code
    If FPS drops below 15, find the bottleneck:
    - Is it MediaPipe detection? â†’ Lower confidence threshold
    - Is it model inference? â†’ Use lighter model
    - Is it drawing? â†’ Reduce overlay complexity

DEBUGGING TIPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… Visualize landmarks before training
    Draw landmarks on images and visually verify they're correct.
    A misaligned landmark â†’ garbage model.

  â˜… Print shapes at every step
    print(features.shape) before model.predict() catches 90% of bugs.

  â˜… Use a webcam test script
    Before debugging complex code, make sure your camera works:
      cap = cv2.VideoCapture(0)
      ret, frame = cap.read()
      print(ret, frame.shape)

  â˜… Save error cases
    When prediction is wrong during live testing, save that frame
    for analysis. It helps you understand failure modes.


================================================================================
6. TUTORIALS, DOCUMENTATION & RESOURCES
================================================================================

OFFICIAL DOCUMENTATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  MediaPipe Hands:
    https://developers.google.com/mediapipe/solutions/vision/hand_landmarker
    â†’ Official guide for hand landmark detection, API reference

  OpenCV Documentation:
    https://docs.opencv.org/4.x/
    â†’ Complete reference for all cv2 functions

  scikit-learn User Guide:
    https://scikit-learn.org/stable/user_guide.html
    â†’ RandomForest, train_test_split, metrics â€” everything we use

  TensorFlow/Keras:
    https://www.tensorflow.org/guide
    https://keras.io/guides/
    â†’ For when we build neural networks in Phase 3/6

  NumPy:
    https://numpy.org/doc/stable/
    â†’ Array operations reference

RECOMMENDED YOUTUBE TUTORIALS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… "Sign Language Detection with Python and Scikit Learn"
    by Computer Vision Engineer (YouTube)
    â†’ This is THE tutorial for our exact approach. Covers:
      data collection, landmark extraction, RandomForest training,
      and real-time classification. Very similar to the Handy project.

  â˜… "Hand Tracking 30 FPS using CPU | OpenCV Python (2021)"
    by Murtaza's Workshop (YouTube)
    â†’ Great for understanding the MediaPipe + OpenCV pipeline

  â˜… "Sign Language Recognition using LSTM"
    by Nicholas Renotte (YouTube)
    â†’ Covers dynamic sign recognition with LSTM
    â†’ Directly relevant to Phase 6

  â˜… "Build a Deep Learning Sign Language Classifier"
    by Sentdex / Nicholas Renotte (YouTube)
    â†’ Full project from data collection to deployment

  â˜… "MediaPipe Hands Documentation / Examples"
    by Google AI (YouTube / GitHub)
    â†’ Official examples and tutorials

DATASETS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… ASL MNIST (Kaggle)
    https://www.kaggle.com/datasets/datamunge/sign-language-mnist
    â†’ 28x28 grayscale images of ASL letters
    â†’ You already have sign_mnist_train.csv locally!

  â˜… ASL Alphabet (Kaggle)
    https://www.kaggle.com/datasets/grassknoted/asl-alphabet
    â†’ 87,000 images of 29 classes (Aâ€“Z + space/delete/nothing)
    â†’ High quality, great for training

  â˜… WLASL (Word-Level American Sign Language)
    https://dxli94.github.io/WLASL/
    â†’ Video dataset of 2000 ASL words
    â†’ For long-term dynamic sign recognition

GITHUB REPOSITORIES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… Handy-Sign-Language-Detection (already on your system)
    â†’ Reference implementation we studied

  â˜… google/mediapipe
    https://github.com/google/mediapipe
    â†’ Official MediaPipe source code and examples

  â˜… sign-language-detector-python (by computervisioneng)
    https://github.com/computervisioneng/sign-language-detector-python
    â†’ Clean implementation of the sklearn approach

PAPERS & ARTICLES (Optional Reading):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â˜… "Real-time Hand Gesture Recognition using MediaPipe"
    â†’ Explains the landmark model architecture

  â˜… "Deep Learning Approaches for Sign Language Recognition: A Survey"
    â†’ Academic overview of methods and state of the art


================================================================================
7. ARCHITECTURE OVERVIEW
================================================================================

Data Flow (Real-Time Inference):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  WEBCAM FRAME
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  HandDetector     â”‚  â† MediaPipe: detect 21 landmarks
  â”‚  (hand_detector)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ landmarks (21 Ã— x,y)
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ FeatureExtractor  â”‚  â† Normalize landmarks to features
  â”‚ (feature_extractor)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ feature vector (42 values)
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ SignClassifier    â”‚  â† RandomForest / Neural Network
  â”‚ (sign_classifier) â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ (label, confidence)
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PredictionSmootherâ”‚  â† deque + Counter (stability)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ stable prediction
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ SentenceBuilder   â”‚  â† Accumulate letters â†’ words â†’ sentences
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ current sentence
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Display / TTS     â”‚  â† Show on screen + optional speech
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Training Pipeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  data/collect_images.py       â†’  Webcam captures saved to data/raw/
            â”‚
            â–¼
  data/extract_landmarks.py    â†’  MediaPipe extracts landmarks â†’ data.pickle
            â”‚
            â–¼
  models/train_model.py        â†’  sklearn trains model â†’ models/saved/model.p
            â”‚
            â–¼
  sign_classifier.py           â†’  Loads model for real-time use


================================================================================
  PRIORITY ORDER SUMMARY â€” WHAT TO DO RIGHT NOW
================================================================================

  1. âœï¸  Create the new file structure (Phase 0) â€” 1 day
  2. ğŸ“¸  Build data collection script (Phase 1) â€” 2 days
  3. ğŸ”¬  Write feature extraction module (Phase 2) â€” 1 day
  4. ğŸ§   Train RandomForest classifier (Phase 3) â€” 2 days
  5. ğŸ”´  Integrate into live webcam feed (Phase 4) â€” 2 days
  6. ğŸ‰  MILESTONE: Real-time sign recognition working!
  7. ğŸ—£ï¸  Add text-to-speech (Phase 5) â€” 2 days
  8. ğŸƒ  Dynamic gesture recognition (Phase 6) â€” 5 days
  9. ğŸ¨  Polish UI and build app (Phase 7) â€” 5 days

  Total estimated time: 3â€“6 weeks of focused development

================================================================================
  END OF ROADMAP â€” Let's build this! ğŸ¤Ÿ
================================================================================
